%\VignetteIndexEntry{SimDesign}
%\VignetteEngine{knitr::knitr}
---
title: "SimDesign"
author: "Phil Chalmers"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_download: true
---

```{r include=FALSE}
options(digits = 3)
set.seed(1)
```

# Power-curve

Power-curves are useful when researchers are interested in the range of power effects when varying different population and sample properties. For instance, researchers may be interested in how power improves as the magnitude of the population mean difference for an independent samples $t$-test increases, or how quickly power improves when increasing the sample size ($N$). The following simulation assumes that the population variances are equal.   

### Define the functions

```{r}
library(SimDesign)
#SimFunctions(comments = FALSE)

### Define design conditions. Here we want a finer range of values for plotting
Design <- createDesign(mean_diff = seq(0, 1, by = .1), 
                       sample_size = c(10, 20, 30))

#--------------------------------------------------------------------

Generate <- function(condition, fixed_objects = NULL) {
  Attach(condition)
  dat1 <- rnorm(sample_size)
  dat2 <- rnorm(sample_size, mean=mean_diff)
  dat <- list(dat1=dat1, dat2=dat2)
  dat
}

Analyse <- function(condition, dat, fixed_objects = NULL) {
  ret <- c(p = t.test(dat$dat1, dat$dat2, 
                      var.equal=TRUE)$p.value)
  ret
}

Summarise <- function(condition, results, fixed_objects = NULL) {
  ret <- EDR(results, alpha = .05)
  ret
}

#--------------------------------------------------------------------

### Run the simulation (using all available cores)
res <- runSimulation(Design, replications=10000, verbose=FALSE, parallel=TRUE,
                     generate=Generate, analyse=Analyse, summarise=Summarise)
```

```{r}
# summary of simulation object
summary(res)

# print results
print(res)
```

A power curve is created by placing the detection rates on a $y$-axis and including different factors on the x-axis and other aesthetics (shading, colours, etc). Here we use `ggplot2` to construct suitable power-curves.

```{r fig.align='center'}
library(ggplot2)
ggplot(res, aes(mean_diff, p, colour=factor(sample_size))) + geom_point() + geom_line() +
    xlab('Mean Difference') + ylab('Detection Rate') + ggtitle('Empirical Power Curves') +
    scale_color_discrete('N')
```

Compare these results to analytic formulas, we can see that the two are essentially coinciding with good accuracy. 

```{r}
library(pwr)
pwr.t.test(d=0.5, n=30, sig.level=0.05, type="two.sample", alternative="two.sided")
subset(res, mean_diff == .5 & sample_size == 30)
pwr.t.test(d=0.5, n=10, sig.level=0.05, type="two.sample", alternative="two.sided")
subset(res, mean_diff == .5 & sample_size == 10)
```

# Power-curve with Bootstrapped Confidence Intervals

In situations where analytic power estimation is not available, and therefore simulations are used as a suitable stand-in, it's important to evaluate the *precision* of the observed power estimates from the simulation study. As such, one reasonable (and extremely general approach) is to apply bootstrapping methodology to the within replication; for simulated power analyses this involves bootstrapping the $R$ replicated $p$-values to obtain suitable non-parametric confidence intervals. 

The following implements the empirical (i.e., basic) bootstrap CI method for the simulation above, and instead plots the point-wise 95% CIs estimates for each condition in the `Design` input. Notice that the replications were reduced from 10000 to 1000 to demonstrate the potential uncertainty when using too few replications when estimating power via simulation.

```{r}
### Run the simulation (using all available cores)
res <- runSimulation(Design, replications=1000, verbose=FALSE, parallel=TRUE,
                     generate=Generate, analyse=Analyse, summarise=Summarise,
                     boot_method = 'basic')
res
```

```{r fig.align='center'}
library(ggplot2)
ggplot(res, aes(mean_diff, p, colour=factor(sample_size))) + geom_point() + geom_line() +
    geom_ribbon(aes(ymin=BOOT_p_2.5, ymax=BOOT_p_97.5, fill=factor(sample_size)), alpha=.3, linetype=0) + 
    xlab('Mean Difference') + ylab('Detection Rate') + ggtitle('Empirical Power Curves with 95% CIs') +
    scale_fill_discrete('N') + scale_colour_discrete('N')
```

Finally, when increasing the replications back to 10000 it become visually clear how precise the original point-estimates  presented in the previous section were. 

```{r}
### Run the simulation (using all available cores)
res <- runSimulation(Design, replications=10000, verbose=FALSE, parallel=TRUE,
                     generate=Generate, analyse=Analyse, summarise=Summarise,
                     boot_method = 'basic')
res
```

```{r fig.align='center'}
library(ggplot2)
ggplot(res, aes(mean_diff, p, colour=factor(sample_size))) + geom_point() + geom_line() +
    geom_ribbon(aes(ymin=BOOT_p_2.5, ymax=BOOT_p_97.5, fill=factor(sample_size)), alpha=.3, linetype=0) + 
    xlab('Mean Difference') + ylab('Detection Rate') + ggtitle('Empirical Power Curves with 95% CIs') +
    scale_fill_discrete('N') + scale_colour_discrete('N')
```


<!-- # Power-curve function estimation -->

<!-- An alternative approach to estimating power curves is to obtain an approximate power function through parametric estimation. This involves estimating the same condition design while varying exactly one axillary condition, potentially with a smaller number of replications, and then fitting the observed proportions using a logistic response function. This is illustrated below.  -->

<!-- At best, however, this is an approximation and is not guaranteed to behave optimally (unlike the previous approach). Therefore, **for serious applications use the above approach instead**. -->

<!-- ```{r} -->
<!-- library(SimDesign) -->
<!-- #SimFunctions(comments = FALSE) -->

<!-- ### Find out parametric power curve for mean_diff = 0.3 -->
<!-- Design <- createDesign(mean_diff = 0.3,  -->
<!--                        sample_size = 5:200) -->

<!-- #-------------------------------------------------------------------- -->

<!-- Generate <- function(condition, fixed_objects = NULL) { -->
<!--     ret <- with(condition, rnorm(sample_size, mean_diff)) -->
<!--     ret -->
<!-- } -->

<!-- Analyse <- function(condition, dat, fixed_objects = NULL) { -->
<!--     ret <- c(reject = t.test(dat)$p.value < .05)  -->
<!--     as.numeric(ret) #return 0/1 rather than FALSE/TRUE -->
<!-- } -->

<!-- #-------------------------------------------------------------------- -->

<!-- ### Run the simulation (note: no summarise function) -->
<!-- reps <- 50 -->
<!-- res <- runSimulation(Design, replications=reps, verbose=FALSE, parallel=FALSE, -->
<!--                      generate=Generate, analyse=Analyse) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- res <- do.call(rbind, res) -->
<!-- head(res) -->
<!-- tail(res) -->

<!-- IV <- rep(Design$sample_size, each=reps) -->
<!-- (mod <- glm(res ~ IV, family = binomial)) -->
<!-- plot(IV, plogis(predict(mod)), ylim = c(0,1), las=1, -->
<!-- 	 type = 'l', xlab = 'Sample size', ylab = 'Power', main = 'Mean difference = 0.3') -->

<!-- # true power function -->
<!-- library(pwr) -->
<!-- out <- pwr.t.test(d=0.3, n=Design$sample_size, sig.level=0.05, type="one.sample", alternative="two.sided") -->
<!-- lines(Design$sample_size, out$power, col='red') -->
<!-- ``` -->

